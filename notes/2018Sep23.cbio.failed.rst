--------------------------------- KUBENETES FAILED (not the way to go)--------------------------------------------- 
Kubenetes Tutorial:
	[mac] ssh -i "~/.ssh/biolinux8_key_pair.pem" ubuntu@ec2-54-213-74-217.us-west-2.compute.amazonaws.com
	[kube] sudo apt install aws-cli
	#FIX kubectl and kops platform and obsolte version issues
	[kube] wget https://github.com/kubernetes/kops/releases/download/1.10.0/kops-linux-amd64
[kube] chmod +x kops-linux-amd64
[kube] sudo mv kops-linux-amd64 /usr/local/bin/kops
	[kube] wget https://storage.googleapis.com/kubernetes-release/release/v1.8.7/bin/linux/amd64/kubectl
	[kube] chmod +x kubectl
	[kube] sudo mv kubectl  /usr/local/bin/kubectl
	[mac] aws s3 mb s3://clusters.k8s.testkube1.vpc   #this avoids aws configure on kube
	[kube] export KOPS_NAME=cluster.k8s.testkube1.vpc
	#FIX: Unable to list AWS regions: NoCredentialProviders: no valid providers in chain. Deprecated.
	[kube] export AWS_ACCESS_KEY_ID=your_aws_key_id
	[kube] export AWS_SECRET_ACCESS_KEY=your_aws_access_key
	#FIX: Could not retrieve location for AWS bucket; occurs if s3 owner and ec2 owner are different
	[mac] aws s3api put-bucket-policy --bucket $KOPS_NAME --policy file://policy.json
	#FIX: kops create cluster throws - error reading SSH key file
	[kube] ssh-keygen
	[kube] cd /home/ubuntu/snap/kops/2
	[kube] ln -s ~/.ssh
	#FIX: hosted zone not found, just make sure hosted zone name is the same as name --dns-zone=testkube1.vpc
	[kube] kops create cluster --cloud=aws --zones=us-west-2b --state=s3://$KOPS_NAME --name=$KOPS_NAME  --dns private
	[kube] kops get cluster $KOPS_NAME --state=s3://$KOPS_NAME
	#FIX: actually build, use and validate the cluster
	[kube] kops update cluster $KOPS_NAME --state=s3://$KOPS_NAME --yes
	#FIX: Error from server (NotAcceptable): unknown (get nodes); make sure kubectl version match 
	#Client Version: version.Info{Major:"1", Minor:"8", GitVersion:"v1.8.7", GitCommit:"b30876a5539f09684ff9fde266fda10b37738c9c", GitTreeState:"clean", #BuildDate:"2018-01-16T21:59:57Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
	#Server Version: version.Info{Major:"1", Minor:"6", GitVersion:"v1.6.13", GitCommit:"14ea65f53cdae4a5657cf38cfc8d7349b75b5512", GitTreeState:"clean", #BuildDate:"2017-11-22T20:19:06Z", GoVersion:"go1.7.6", Compiler:"gc", Platform:"linux/amd64"}
	[kube] kubectl get nodes
	[kube] kops validate cluster --state=s3://$KOPS_NAME
	[kube] kops delete cluster --name=cluster.k8s.testkube1.vpc --state=s3://cluster.k8s.testkube1.vpc --yes #release all pods

Kubenetes Hello-world: 
	[kube] kubectl run hello-world --replicas=5 --labels="run=load-balancer-example" --image=gcr.io/google-samples/node-hello:1.0  --port=8080
[kube] kubectl get deployments hello-world
	[kube] kubectl describe deployments hello-world
	[kube] kubectl get replicasets
[kube] kubectl describe replicasets
	[kube] kubectl expose deployment hello-world --type=LoadBalancer --name=my-service
	[kube] kubectl get services my-service
	[kube] kubectl describe services my-service
	#FIX: is not authorized to perform: iam:CreateServiceLinkedRole on resource
	[kube] aws iam create-service-linked-role --aws-service-name "elasticloadbalancing.amazonaws.com"
	[kube] kubectl get pods --output=wide
	[kube] curl http://aac4f8754c1dd11e8921b02b541555c2-1915315496.us-west-2.elb.amazonaws.com:8080
	#check it works!, now clean up
	[kube] kubectl delete services my-service
	[kube] kubectl delete deployment hello-world

Helm:
	[kube] wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-rc.4-linux-amd64.tar.gz
	[kube] tar -zxvf helm-v2.11.0-rc.4-linux-amd64.tar.gz
	[kube] sudo mv helm tiller /usr/local/bin/
	[kube] chmod +x /usr/local/bin/helm
	[kube] chmod +x /usr/local/bin/tiller
	[kube] helm init --service-account tiller
	[kube] kubectl config current-context
	[kube] helm init --upgrade
	[kube] helm install stable/mysql
	
Helm Nginx:
	[kube] helm install docs/examples/nginx
	[kube] kubectl get deployments
	#Note steely-hog-nginx is the deployed helm chart
	[kube] kubectl describe deployments steely-hog-nginx
	#Note port 80 is used for nginx
	[kube] kubectl get pod steely-hog-nginx
	#Note steely-hog-nginx-529935090-9jmqr is running
	[kube] kubectl expose deployment steely-hog-nginx --type=LoadBalancer --name=my-service
	#expose it as service and 
	[kube] kubectl get services my-service
	[kube] kubectl describe services my-service
	#wait minutes to confirm it is running, see Ingress external IP
	[mac] curl http://ac6faf6d3c29e11e8921b02b541555c2-624860657.us-west-2.elb.amazonaws.com:80
	#check it works!, now clean up
	[kube] kubectl delete services my-service
	[kube] kubectl delete deployment steely-hog-nginx
	
Helm Mongo:
	[kube] helm install --name cbioportal-session-service-mongo --set persistence.size=10Gi stable/mongodb
	[kube] helm ls --all
	#connect to mongodb 
	[kube] export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace default cbioportal-session-service-mongo-mongodb -o jsonpath="{.data.mongodb-root-password}" | base64 --decode)
	[kube] kubectl run --namespace default cbioportal-session-service-mongo-mongodb-client --rm --tty -i --image bitnami/mongodb --command -- mongo admin --host cbioportal-session-service-mongo-mongodb -u root -p $MONGODB_ROOT_PASSWORD
	#Connect from outside; NAME=cbioportal-session-service-mongo-mongodb-447933458-51vhl
	[kube] kubectl get pods
	[kube] kubectl port-forward --namespace default cbioportal-session-service-mongo-mongodb-447933458-51vhl 27017:27017 &
	#check mongodb works!
	[kube] mongo --host 127.0.0.1 -p $MONGODB_ROOT_PASSWORD
	[kube] kubectl apply -f session-service/session_service.yaml
	#check session service works! by swagger-ui.html, details in https://github.com/cBioPortal/session-service; despite of warning
	[kube] curl http://ae1a69d8bc2a411e8921b02b541555c2-560138593.us-west-2.elb.amazonaws.com:8888/api/sessions/my_portal/main_session/
	
Helm Mysql ?:
	[kube] ssh -i "~/.ssh/biolinux8_key_pair.pem" ubuntu@ec2-54-213-74-217.us-west-2.compute.amazonaws.com
	[kube] sudo apt-get install mysql-client -y #install mysql
	#FIX: mysql Pending after change node size, t3.xlarge not supported. t2.xlarge OK
	[kube] kops edit ig --name=cluster.k8s.testkube1.vpc --state=s3://cluster.k8s.testkube1.vpc nodes
	[kube] kops edit ig --name=cluster.k8s.testkube1.vpc --state=s3://cluster.k8s.testkube1.vpc master-us-west-2b
	[kube] kops update cluster --name=cluster.k8s.testkube1.vpc --state=s3://cluster.k8s.testkube1.vpc --yes
	[kube] kops rolling-update cluster --name=cluster.k8s.testkube1.vpc --state=s3://cluster.k8s.testkube1.vpc --yes
	#FIX:  Access denied for user ‘cbio_user’ or 'root'
	[kube] kubectl get pod #Pod is a VM: cbioportal-prod-db-mysql-2774845561-zw73x
	[kube] kubectl exec  -it cbioportal-prod-db-mysql-2774845561-zw73x  /bin/sh
	[kube] echo $MYSQL_ROOT_PASSWORD
	[kube] mysql -u root -p
	### up to here, to be continued
	[kube] kubectl expose deployment cbioportal-prod-db-mysql --type=LoadBalancer --name=cbioportal-prod-db-mysql-expose
	[kube] kubectl get services cbioportal-prod-db-mysql-expose
	[kube] kubectl describe services cbioportal-prod-db-mysql-expose
	[kube] kubectl get deployments cbioportal-session-service
	[kube] kubectl get pod
	[kube] kubectl expose deployment cbioportal-session-service --type=LoadBalancer --name=cbioportal-session-service-expose
	[kube] kubectl get services cbioportal-session-service-expose
	[kube] kubectl describe services cbioportal-session-service-expose
	#Error: Whitelabel Error Page
	#FIX: Get mysql password, as specified in cbioportal_mysql_db_values.yml, "P@ssword1"; 
	#[kube] helm upgrade --install -f cbioportal_mysql_db_values.yml cbioportal-prod-db stable/mysql
	#[kube] kubectl get secret --namespace default cbioportal-prod-db-mysql -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo
	#[kube] kubectl get secret --namespace default cbioportal-prod-db-mysql -o jsonpath="{.data.mysql-password}" | base64 --decode; echo
	
Monitor [Failed]:
	[kube]: helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/
	#Fix: Error: release prometheus-operator failed: clusterroles.rbac.authorization.k8s.io "prometheus-operator" is forbidden: attempt to grant extra privileges:
	[kube]: 
	#delete tiller
	kubectl -n kube-system delete deployment tiller-deploy
	kubectl delete clusterrolebinding tiller
	kubectl -n kube-system delete serviceaccount tiller
	Error: ruleResolutionErrors=[clusterroles.rbac.authorization.k8s.io "cluster-admin" not found]
